{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### This is a copu of ml-data-preprocessing, written to help with scaffolding for the query multiplexer.\n",
    "### 02-ml-data-preprocessing is still the main preprocessing file\n",
    "\n",
    "### ML Data pre-processing\n",
    "This notebook is for loading and cleaning the data that will be used to train the ML on.\n",
    "Things like patient heart rate and blood pressure readings that occurred around the time of the administration of the second dose \n",
    "\n",
    "It should persist the data into the \"out\" directory to be consumed by the ml training notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f8191d46f8661e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import root_config as rc\n",
    "from detectdd import config\n",
    "import pandas as pd\n",
    "\n",
    "rc.configure()\n",
    "\n",
    "from detectdd.auth_bigquery import BigQueryClient\n",
    "from detectdd.serializer import Serializer\n",
    "\n",
    "print(\"Loading cohort\")\n",
    "\n",
    "try:\n",
    "    serializer = Serializer()\n",
    "    cohort_with_icd = serializer.read_cohort()  # need to run 01-cohort.ipynb to produce the cohort\n",
    "    print(len(cohort_with_icd))\n",
    "    cohort_without_icd = serializer.read_cohort_with_no_icd()\n",
    "    print(len(cohort_without_icd))\n",
    "    cohort = pd.concat([cohort_with_icd.head(10000), cohort_without_icd.head(10000)])\n",
    "except FileNotFoundError:\n",
    "    raise Exception(\"Need to run [01-cohort.ipynb] at least once to create the cohort file in the /out directory\")\n",
    "\n",
    "big_query = BigQueryClient.auth()\n",
    "\n",
    "\n",
    "from detectdd.query_multiplexer import WhereClauseGenerator\n",
    "from detectdd.query_multiplexer import QueryMultiplexer\n",
    "import pandas as pd\n",
    "from detectdd.auth_bigquery import BigQueryClient\n",
    "\n",
    "cohort.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f01d003a2af2b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a Serializer class that handles reading your saved cohort data\n",
    "serializer = Serializer()\n",
    "\n",
    "# Extract unique subject_ids from the cohort data\n",
    "subject_ids = cohort['subject_id'].unique()\n",
    "\n",
    "# Convert the list of subject_ids to a format suitable for SQL query\n",
    "subject_id_str = ', '.join([str(id) for id in subject_ids])\n",
    "# print(subject_id_str)\n",
    "# Now, let's proceed to fetch the vital signs for these subject_ids from MIMIC\n",
    "\n",
    "query_multiplexer = QueryMultiplexer(big_query)\n",
    "\n",
    "# Write a SQL query to fetch the required vitals where the subject_ids are in your cohort\n",
    "query = \"\"\"\n",
    "SELECT stay_id, subject_id, charttime, heart_rate, sbp, dbp, mbp\n",
    "FROM `physionet-data.mimiciv_derived.vitalsign`\n",
    "WHERE ($where) \n",
    "    AND (heart_rate IS NOT NULL OR sbp IS NOT NULL OR dbp IS NOT NULL OR mbp IS NOT NULL)\n",
    "\"\"\"\n",
    "\n",
    "# query = f\"\"\"\n",
    "# SELECT subject_id, heart_rate, sbp, dbp, mbp\n",
    "# FROM `physionet-data.mimiciv_derived.vitalsign`\n",
    "# WHERE subject_id IN ({subject_id_str}) limit 100\"\"\"\n",
    "\n",
    "where_fragment = \"(stay_id= $stay_id AND charttime > DATETIME_ADD('$dose_b_time', INTERVAL -720 MINUTE) AND charttime < DATETIME_ADD('$dose_b_time', INTERVAL 720 MINUTE))\"\n",
    "\n",
    "multimap_data = {k: v.tolist() for k, v in cohort.groupby('stay_id')['dose_b_time']}\n",
    "results = query_multiplexer.multiplex_query(query, multi_map_data=multimap_data,\n",
    "                                            where_clause=WhereClauseGenerator(where_fragment, \"stay_id\", \"dose_b_time\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d647680bc5e6edd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the query\n",
    "vitals_data = results\n",
    "vitals_data.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "474a0506f1d2389f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Now you have the vital signs data for the patients in your cohort.\n",
    "# You can proceed to clean this data as needed and use it for further analysis or machine learning model training.\n",
    "\n",
    "# If you need to save this data locally for further use in your ML training notebook, you can do so like this:\n",
    "# vitals_data.to_csv(config.out_dir / 'vitals_data.csv', index=False)\n",
    "vitals_data.to_csv(config.out_dir / 'vitals_data-abc-before-and-after.csv', index=False)\n",
    "\n",
    "vitals_data.head(100)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16f8d50141f7a3ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7c015c4efeb05f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
