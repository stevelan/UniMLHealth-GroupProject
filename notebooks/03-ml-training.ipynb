{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from detectdd import config\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import root_config as rc\n",
    "import pandas as pd\n",
    "from detectdd.serializer import Serializer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rc.configure()\n",
    "\n",
    "bp_df = pd.read_csv(config.out_dir / 'vitals_data-10.csv')\n",
    "print(bp_df.dtypes)\n",
    "\n",
    "bp_df[\"charttime\"] = bp_df[\"charttime\"].astype(\"datetime64[s]\")\n",
    "bp_df[\"dose_b_time\"] = bp_df[\"dose_b_time\"].astype(\"datetime64[s]\")\n",
    "\n",
    "bp_df = bp_df.sort_values(by=[\"stay_id\", \"dose_b_time\", \"charttime\"])\n",
    "\n",
    "try:\n",
    "    serializer = Serializer()\n",
    "    cohort_with_icd = serializer.read_cohort()  # need to run 01-cohort.ipynb to produce the cohort\n",
    "    print(len(cohort_with_icd))\n",
    "    cohort_without_icd = serializer.read_cohort_with_no_icd()\n",
    "    print(len(cohort_without_icd))\n",
    "    cohort = pd.concat([cohort_with_icd, cohort_without_icd])\n",
    "except FileNotFoundError:\n",
    "    raise Exception(\"Need to run [01-cohort.ipynb] at least once to create the cohort file in the /out directory\")\n",
    "\n",
    "unique_subject_num_icd_codes = cohort[[\"subject_id\", \"num_icd_codes\"]].drop_duplicates(subset=\"subject_id\")\n",
    "print(unique_subject_num_icd_codes)\n",
    "print(bp_df)\n",
    "\n",
    "bp_df = bp_df.drop_duplicates()\n",
    "\n",
    "bp_df = pd.merge(left=bp_df, right=unique_subject_num_icd_codes, how=\"inner\", on='subject_id')\n",
    "\n",
    "bp_df[\"has_icd\"] = ~bp_df['num_icd_codes'].isna()\n",
    "bp_df[\"has_icd\"].describe()\n",
    "bp_df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bp_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "530e967ff3330a62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bp_df[bp_df['stay_id'] == 30031264].sort_values(by='charttime')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb19c3278f1951c2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T13:30:03.112051400Z",
     "start_time": "2023-10-26T13:30:02.901022Z"
    }
   },
   "id": "71bb824da43eebaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "serializer = Serializer()\n",
    "bp_df_no_icd = serializer.read_bp_results(\"10-no-icd\")\n",
    "bp_df_no_icd[\"has_icd\"] = 0\n",
    "bp_df_no_icd\n",
    "df = bp_df\n",
    "# df= pd.concat([bp_df, bp_df_no_icd])\n",
    "# df = pd.DataFrame(df.head(5000)) #TODO remove me"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30869887ad752c36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"charttime\"] = df[\"charttime\"].astype(\"datetime64[s]\")\n",
    "df[\"dose_b_time\"] = df[\"dose_b_time\"].astype(\"datetime64[s]\")\n",
    "\n",
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f74b3246f2c2e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "## convert chart_time to timestep relative to dose b time\n",
    "df[\"timestep\"] = df[\"charttime\"] - df[\"dose_b_time\"]\n",
    "## and convert to float64 for tensor flow\n",
    "df['timestep_float'] = df['timestep'].dt.total_seconds().astype('float64')\n",
    "\n",
    "data_arrays = [\"heart_rate\", \"sbp\", \"dbp\", \"mbp\"]\n",
    "for data in data_arrays:\n",
    "    df[data] = df[data].astype('float64')\n",
    "\n",
    "print(df.dtypes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbdedf0ecd5747bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df[[\"stay_id\", \"timestep\", \"timestep_float\", \"heart_rate\", \"sbp\", \"dbp\", \"mbp\", \"dose_b_time\", \"charttime\",\n",
    "         \"has_icd\"]].sort_values([\"stay_id\", \"dose_b_time\", \"timestep\"])\n",
    "df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9425cee888566589"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpolate\n",
    "The data has missing values, use pandas built in interpolation to fill those based on the timestamp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2108b67729841627"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-26T13:29:22.035511Z"
    }
   },
   "id": "a849a0e0ee1357b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(df.dtypes)\n",
    "# df = df.reset_index()\n",
    "df.set_index('charttime', inplace=True)\n",
    "df.groupby([\"stay_id\", \"has_icd\", \"dose_b_time\"])\n",
    "\n",
    "\n",
    "def interpolate_group(group):\n",
    "    if group.name in data_arrays:\n",
    "        # Perform interpolation for the group\n",
    "        group = group.interpolate(method='time')\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Apply the interpolation function to each group\n",
    "df = df.apply(interpolate_group).reset_index()\n",
    "\n",
    "\n",
    "def list_of_floats(x):\n",
    "    return x.astype(float).tolist()\n",
    "\n",
    "\n",
    "group_df = df[[\"stay_id\", \"dose_b_time\", \"timestep_float\", \"heart_rate\", \"sbp\", \"dbp\", \"mbp\", \"has_icd\"]]\n",
    "print(df.dtypes)\n",
    "grouped_df = group_df  \n",
    "\n",
    "print(grouped_df.dtypes)\n",
    "\n",
    "print(grouped_df.shape)\n",
    "grouped_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "973f50807e404908"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove outliers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f573370ebcbbac0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reduce the size of the data arrays to the p999 to get rid of the extreme outliers\n",
    "length = pd.DataFrame()\n",
    "length[\"val\"] = [len(array) for array in grouped_df.groupby([\"stay_id\", \"dose_b_time\"])[\"mbp\"]]\n",
    "print(length.describe(percentiles=[0.01, 0.025, 0.25, 0.5, 0.75, 0.975, 0.99, 0.999]))\n",
    "print(grouped_df.dtypes)\n",
    "max_size = 90  # p999\n",
    "\n",
    "data_array_with_time = data_arrays.copy()\n",
    "data_array_with_time.append(\"timestep_float\")\n",
    "print(data_array_with_time)\n",
    "print(data_arrays)\n",
    "\n",
    "\n",
    "def resize(s):\n",
    "    s.head(max_size)\n",
    "\n",
    "\n",
    "grouped_df = grouped_df.groupby([\"stay_id\", \"dose_b_time\"]).head(max_size)\n",
    "\n",
    "max_sequence_length = max(len(array) for array in grouped_df.groupby([\"stay_id\", \"dose_b_time\"])[\"sbp\"])\n",
    "print(max_sequence_length)\n",
    "print(grouped_df.dtypes)\n",
    "grouped_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7577b32922b217c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unify timesteps\n",
    "The time measurement data varies per dosage, this step aligns all the time steps to 10 minute intervals, and interpolates between our \n",
    "existing time measures to produce 12 hours of 10 minute timestaps "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d458e55c120a81b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first fill blank values through interpellation\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def interpolate_row(row):\n",
    "    vals = pd.DataFrame()\n",
    "    time_steps = np.array(row[\"timestep_float\"])\n",
    "    common_time_interval = range(0, 12 * 60, 10)\n",
    "\n",
    "    for data_array in data_arrays:\n",
    "        interp_func = interp1d(time_steps, row[data_array], kind='next', fill_value='extrapolate')\n",
    "        vals[\"intr-\" + data_array] = interp_func(common_time_interval)\n",
    "\n",
    "    vals[\"common_timestep\"] = common_time_interval\n",
    "\n",
    "    return vals\n",
    "\n",
    "\n",
    "# Apply the function to the specific subset of rows\n",
    "interpolated_df = grouped_df.groupby(['stay_id', 'has_icd', 'dose_b_time']).apply(interpolate_row)\n",
    "print(interpolated_df)\n",
    "print(grouped_df.dtypes)\n",
    "interpolated_df.head(50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bf9396b30fdfa6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalise and scale data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26913a26a7502d87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "# \n",
    "features = ['intr-heart_rate', 'intr-sbp', 'intr-dbp', 'intr-mbp']\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the column\n",
    "\n",
    "for feature in features:\n",
    "    interpolated_df[feature] = scaler.fit_transform(interpolated_df[[feature]])\n",
    "\n",
    "print(interpolated_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87733823e74147e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(grouped_df.dtypes)\n",
    "\n",
    "interpolated_df.groupby(['stay_id', 'dose_b_time']).count().describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "637472fd5703a23e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "interpolated_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24c688fe985f190d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import shape\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "target = 'has_icd'\n",
    "\n",
    "# Extract features and timestep data\n",
    "\n",
    "interpolated_df = interpolated_df.reset_index()\n",
    "timesteps = interpolated_df.groupby([\"stay_id\", \"dose_b_time\"])['common_timestep'].count()\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_sequence_length = max(timesteps)\n",
    "print(f\"Max sequence length {max_sequence_length}\")\n",
    "\n",
    "max_dosages = df.groupby(\"stay_id\")[\"dose_b_time\"].nunique().max()\n",
    "print(f\"Max dosages {max_dosages}\")\n",
    "\n",
    "# Organize data by stay_id\n",
    "stay_ids = interpolated_df['stay_id'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_sequences = []\n",
    "zeros = np.zeros((max_sequence_length, len(features)))\n",
    "\n",
    "cols = ['stay_id', 'dose_b_time'] + features\n",
    "print(cols)\n",
    "xs = interpolated_df[cols].values\n",
    "shape(xs)\n",
    "\n",
    "# Initialize an empty list to store arrays\n",
    "result = []\n",
    "\n",
    "# Iterate over unique values of key1\n",
    "invalid_stays = []\n",
    "for stay_id in stay_ids:\n",
    "    # Filter the DataFrame for the current key1\n",
    "    df_stays = interpolated_df[interpolated_df[\"stay_id\"] == stay_id]\n",
    "\n",
    "    # Create a list to store arrays for the current key1\n",
    "    stays_array = []\n",
    "\n",
    "    unique_doses = df_stays['dose_b_time'].unique()\n",
    "    # Iterate over unique values of key2 within the current key1\n",
    "    for dose_b_time in unique_doses:\n",
    "    # Filter the DataFrame for the current key2\n",
    "        df_dose = df_stays[df_stays[\"dose_b_time\"] == dose_b_time]\n",
    "    \n",
    "        # Extract the \"val\" columns for the current key2\n",
    "        val_columns = df_dose[features].values\n",
    "        \n",
    "        # Append the extracted values as an array to the list\n",
    "        if ~np.any(np.isnan(val_columns)):\n",
    "            stays_array.append(val_columns)\n",
    "        else:\n",
    "            invalid_stays.append(stay_id)\n",
    "            print(f\"Found array values with NaN {stay_id} - {val_columns}\")\n",
    "        \n",
    "        \n",
    "    # Pad out the second dimension with zeros\n",
    "    while len(stays_array) < max_dosages:\n",
    "        stays_array.append(zeros)\n",
    "    \n",
    "        # Convert the list of arrays to a NumPy array for the current key1\n",
    "    stays_array = np.array(stays_array)\n",
    "    y_sequences.append(df_stays[target].unique()[0])\n",
    "    # Append the 3D array for the current key1 to the result list\n",
    "    result.append(stays_array)\n",
    "    \n",
    "\n",
    "# Convert the result list to a NumPy array\n",
    "X_sequences = np.array(result)\n",
    "\n",
    "shape(X_sequences)\n",
    "\n",
    "original_shape = X_sequences.shape\n",
    "X_sequences = X_sequences.reshape(original_shape[0], original_shape[1] * original_shape[2], original_shape[3])\n",
    "\n",
    "print(shape(X_sequences))\n",
    "print(shape(y_sequences))\n",
    "\n",
    "print(f\"Found {len(invalid_stays)} - {invalid_stays}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a076c674ca138cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_sequences = np.array(y_sequences).astype(int)\n",
    "print(shape(X_sequences))\n",
    "\n",
    "X = X_sequences[:3000]\n",
    "y = y_sequences[:3000]\n",
    "\n",
    "print(shape(X))\n",
    "\n",
    "print(shape(y))\n",
    "test = pd.DataFrame(np.isnan(X.flatten()))\n",
    "\n",
    "print(f\"Found {len(invalid_stays)} - {invalid_stays}\")\n",
    "test[test[0]==True]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96411bfc952ab4a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_df.head(2000).reset_index().groupby('stay_id')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1af354846d127917"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "#Adjust the size of the testing set: we'll use 10% of the entire data. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "#Check the number of columns (features):\n",
    "print(len(X_train))\n",
    "print(shape(X_train))\n",
    "print(shape(X_test))\n",
    "print(shape(y_train))\n",
    "print(shape(y_test))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba5b915cab2f0376"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, BatchNormalization, LayerNormalization, Dense\n",
    "\n",
    "print(max_sequence_length)\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "max_feature_dim = 4\n",
    "LAYERS = [32,32,1]\n",
    "\n",
    "model.add(keras.layers.Masking(mask_value=0))\n",
    "model.add(LSTM(64))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c1ca833717a939a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24fd5efbe0bd6613"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = model.fit(X_train, y_train, epochs=10, batch_size=200,\n",
    "                       verbose=1)  #set verbose = 1 to see the fitting process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4263eb1674d3a26c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Accuracy over the epochs\n",
    "plt.plot(classifier.history['accuracy'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('model accuracy')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss over the epochs\n",
    "plt.plot(classifier.history['loss'])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('model loss')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd261ac8e14dba93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the keras model\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print(round(loss,3),round(accuracy,3))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88743f97d504f1ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(round(loss,3),round(accuracy,3))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20ceebc152732763"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# #Predict the testing set\n",
    "predictions = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "#Accuracy classification score\n",
    "acc = float(round(metrics.accuracy_score(y_test, predictions),3))\n",
    "\n",
    "#Compute the balanced accuracy.\n",
    "bacc = float(round(metrics.balanced_accuracy_score(y_test, predictions),3))\n",
    "\n",
    "#Compute the Matthews correlation coefficient (MCC)\n",
    "mcc = float(round(metrics.matthews_corrcoef(y_test, predictions),3))\n",
    "\n",
    "#Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "f1 = float(round(metrics.f1_score(y_test, predictions),3))\n",
    "\n",
    "#Show results as a DataFrame:\n",
    "results = {'Accuracy' : [acc], 'Balanced Accuracy' : [bacc], 'MCC' : [mcc], 'F1-Score' : [f1]}\n",
    "df_results = pd.DataFrame.from_dict(data = results, orient='columns')\n",
    "print(df_results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc7ff17b9ec5650"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7aac5e9987a8550f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
